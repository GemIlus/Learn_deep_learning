{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {
        "id": "Awv7UK_E6f83"
      },
      "outputs": [],
      "source": [
        "train_cat_dir='train/cats/'\n",
        "train_dog_dir='train/dogs/'\n",
        "valid_cat_dir='validation/cats/'\n",
        "valid_dog_dir='validation/dogs/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqfiRE3vDBQr"
      },
      "source": [
        "### [TODO 1] Xây dựng mô hình Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {
        "id": "xchCSGIzDzbz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(BatchNormalization())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(MaxPooling2D(pool_size=(2,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Dropout(0.25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Conv2D(64,(3,3),activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(BatchNormalization())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(MaxPooling2D(pool_size=(2,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Dropout(0.25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Conv2D(128,(3,3),activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(BatchNormalization())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(MaxPooling2D(pool_size=(2,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Dropout(0.25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Dense(512,activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(BatchNormalization())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Dropout(0.5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.add(Dense(2,activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_51 (Conv2D)          (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization_41 (Ba  (None, 148, 148, 32)      128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_52 (MaxPooli  (None, 74, 74, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 74, 74, 32)        0         \n",
            "                                                                 \n",
            " conv2d_52 (Conv2D)          (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_42 (Ba  (None, 72, 72, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_53 (MaxPooli  (None, 36, 36, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 36, 36, 64)        0         \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_43 (Ba  (None, 34, 34, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_54 (MaxPooli  (None, 17, 17, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 17, 17, 128)       0         \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 36992)             0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 512)               18940416  \n",
            "                                                                 \n",
            " batch_normalization_44 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19037634 (72.62 MB)\n",
            "Trainable params: 19036162 (72.62 MB)\n",
            "Non-trainable params: 1472 (5.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "earlystop = EarlyStopping(patience = 10)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "callbacks = [earlystop,learning_rate_reduction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rotation_range=15, rescale=1./255, shear_range=0.1, zoom_range=0.2, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2001 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory('train', target_size=(150,150), class_mode='categorical', batch_size=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = validation_datagen.flow_from_directory('validation', target_size=(150,150), class_mode='categorical', batch_size=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kien bui\\AppData\\Local\\Temp\\ipykernel_5420\\4220884401.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history=model.fit_generator(train_generator, epochs=15, steps_per_epoch=133, validation_data=validation_generator, verbose=1,validation_steps=67,callbacks=callbacks)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "133/133 [==============================] - 80s 590ms/step - loss: 0.9838 - accuracy: 0.5775 - val_loss: 0.7180 - val_accuracy: 0.5310 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "133/133 [==============================] - 78s 588ms/step - loss: 0.7848 - accuracy: 0.6083 - val_loss: 0.7915 - val_accuracy: 0.5560 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "133/133 [==============================] - 78s 586ms/step - loss: 0.6983 - accuracy: 0.6365 - val_loss: 0.7401 - val_accuracy: 0.6200 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "133/133 [==============================] - 79s 592ms/step - loss: 0.6472 - accuracy: 0.6737 - val_loss: 0.6236 - val_accuracy: 0.6710 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "133/133 [==============================] - 86s 647ms/step - loss: 0.6209 - accuracy: 0.6788 - val_loss: 0.6401 - val_accuracy: 0.6580 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.6989\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "133/133 [==============================] - 84s 628ms/step - loss: 0.6091 - accuracy: 0.6989 - val_loss: 0.7263 - val_accuracy: 0.6520 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "133/133 [==============================] - 80s 600ms/step - loss: 0.5664 - accuracy: 0.7205 - val_loss: 0.5191 - val_accuracy: 0.7420 - lr: 5.0000e-04\n",
            "Epoch 8/15\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.5464 - accuracy: 0.7452 - val_loss: 0.4835 - val_accuracy: 0.7700 - lr: 5.0000e-04\n",
            "Epoch 9/15\n",
            "133/133 [==============================] - 79s 596ms/step - loss: 0.5261 - accuracy: 0.7578 - val_loss: 0.5033 - val_accuracy: 0.7580 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.7477\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "133/133 [==============================] - 79s 595ms/step - loss: 0.5176 - accuracy: 0.7477 - val_loss: 0.5026 - val_accuracy: 0.7680 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "133/133 [==============================] - 79s 594ms/step - loss: 0.4913 - accuracy: 0.7669 - val_loss: 0.5374 - val_accuracy: 0.7530 - lr: 2.5000e-04\n",
            "Epoch 12/15\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.4771 - accuracy: 0.7754\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.4771 - accuracy: 0.7754 - val_loss: 0.5060 - val_accuracy: 0.7640 - lr: 2.5000e-04\n",
            "Epoch 13/15\n",
            "133/133 [==============================] - 83s 626ms/step - loss: 0.4674 - accuracy: 0.7744 - val_loss: 0.4839 - val_accuracy: 0.7810 - lr: 1.2500e-04\n",
            "Epoch 14/15\n",
            "133/133 [==============================] - 79s 597ms/step - loss: 0.4613 - accuracy: 0.7860 - val_loss: 0.4614 - val_accuracy: 0.7770 - lr: 1.2500e-04\n",
            "Epoch 15/15\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.4569 - accuracy: 0.7946\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "133/133 [==============================] - 80s 598ms/step - loss: 0.4569 - accuracy: 0.7946 - val_loss: 0.4531 - val_accuracy: 0.7770 - lr: 1.2500e-04\n"
          ]
        }
      ],
      "source": [
        "history=model.fit_generator(train_generator, epochs=15, steps_per_epoch=133, validation_data=validation_generator, verbose=1,validation_steps=67,callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtmVP00mDbLG"
      },
      "source": [
        "### [TODO 2] Thực hiện training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {
        "id": "TbLLWcr7Dzuu"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_iga1kDDnIF"
      },
      "source": [
        "### [TODO 3] Thực hiện load file và dự đoán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 552,
      "metadata": {
        "id": "MqyiEs7bD0dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n",
            "D:/cart2.jpg is a cat\n"
          ]
        }
      ],
      "source": [
        "image_path = \"D:/cart2.jpg\"\n",
        "try:\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Convert image to RGB if it has more than 3 channels\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "    \n",
        "    img = img.resize((150, 150))  # Resize để phù hợp với kích thước đầu vào của mô hình\n",
        "    x = np.array(img) / 255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    # Thực hiện dự đoán\n",
        "    classes = model.predict(x)\n",
        "\n",
        "    # In kết quả\n",
        "    if classes[0][1] > 0.5:\n",
        "        print(f\"{image_path} is a dog\")\n",
        "    else:\n",
        "        print(f\"{image_path} is a cat\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {image_path}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpKi3UIUDuZo"
      },
      "source": [
        "### Hiển thị độ chính xác của mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {
        "id": "-qXTOctxDsGZ"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'acc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32md:\\LearnDL\\cat_dog\\cats_and_dogs_filtered\\5_challenge.ipynb Cell 37\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#-----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Retrieve a list of list results on training and test data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# sets for each training epoch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#-----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m acc      \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39;49mhistory[     \u001b[39m'\u001b[39;49m\u001b[39macc\u001b[39;49m\u001b[39m'\u001b[39;49m ]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m val_acc  \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[ \u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m ]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LearnDL/cat_dog/cats_and_dogs_filtered/5_challenge.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss     \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[    \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m ]\n",
            "\u001b[1;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'acc' ]\n",
        "val_acc  = history.history[ 'val_acc' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
